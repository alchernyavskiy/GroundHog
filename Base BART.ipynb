{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get splits for BART training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_token = '</s>'\n",
    "ds_suff = 'short' #'filt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_bart_df(multi_df_path, columns, target):\n",
    "    multi_df = pd.read_csv(multi_df_path, sep='\\t')\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df['@'.join(columns)] = multi_df[columns].apply(lambda x: f' {sep_token} '.join(x), axis=1)\n",
    "    result_df[target] = multi_df[target]\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"response\"\n",
    "source_cols = [\"history\", \"title\", \"grounding\"]\n",
    "\n",
    "for part in ['train', 'val']:\n",
    "    base_df = get_base_bart_df(f'bart_input/{part}_reddit_dial_df_{ds_suff}.csv', source_cols, target_col)\n",
    "    base_df.to_csv(f'bart_input/{part}_reddit_dial_df_base_{ds_suff}__{\"-\".join(source_cols)}__{target_col}.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'bart_input/train_reddit_dial_df_{ds_suff}.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s1> <u1> <to:u1> I don’t think Henry is going to play a different DC character. His instagram post reads like he’s done with DC films, not that he’s going to be in something else just not as Superman. </s> <s2> <u2> <to:u1> At this point hasn't WB burnt the bridge? Toying with the character for years he finally gets welcomed back and now he gets kicked out again?? </s> <s1> <u3> <to:u2> Yup. They burnt that bridge into ashes at this point. It’s interesting to see the difference in tone between James Gunn’s and Henry’s statements. James makes it sound like it was positive. “We had a great meeting” he says. But Henry’s sounds very sad and disappointed. He most definitely did not describe it as a positive meeting. </s> <s3> <u4> <to:u1> What's james gonna say, that he completely fucked Henry over? </s> <s4> <u5> <to:u4> The Rock did that. It was up to Gunn to make the tough call, that’s all. </s> <s5> <u6> <to:u1> I don’t think Henry is going to play a different DC character. His instagram post reads like he’s done with DC films, not that he’s going to be in something else just not as Superman.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['history'].values[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train base BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set by hands using the results above\n",
    "source_lens = {\n",
    "    'history': 600,\n",
    "    'history_aug': 600,\n",
    "    'history_amr': 1024,\n",
    "    'history_discourse': 40,\n",
    "    'addr_amr': 300,\n",
    "    'response': 160,\n",
    "    'response_aug':  160,\n",
    "    'grounding': 850,\n",
    "    'title': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 4\n",
    "learning_rate = 3e-5\n",
    "batch_size = 4\n",
    "gradient_accumulation_steps = 2\n",
    "#target_col = \"response_aug\"\n",
    "target_col = \"response\"\n",
    "source_cols = [\"history\", \"title\", \"grounding\"] #[\"history_aug\", \"title\", \"grounding\"] # in that order, last will be truncateds\n",
    "text_column = '@'.join(source_cols)\n",
    "\n",
    "max_source_length = min(1024, sum([source_lens[c] for c in source_cols]))\n",
    "max_target_length = source_lens[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = f'bart_input/train_reddit_dial_df_base_filt__{\"-\".join(source_cols)}__{target_col}.csv'\n",
    "val_fn = f'bart_input/val_reddit_dial_df_base_filt__{\"-\".join(source_cols)}__{target_col}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoint/base_bart_bs8_4ep_lr3e-05__from:history-title-grounding___to:response'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = f\"checkpoint/base_bart_bs{batch_size*gradient_accumulation_steps}_{n_epochs}ep_lr{learning_rate}__from:{'-'.join(source_cols)}___to:{target_col}\"\n",
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "06/14/2023 16:50:39 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "06/14/2023 16:50:39 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=checkpoint/base_bart_bs8_4ep_lr3e-05__from:history-title-grounding___to:response/runs/Jun14_16-50-39_cn-015,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=4.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=checkpoint/base_bart_bs8_4ep_lr3e-05__from:history-title-grounding___to:response,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=checkpoint/base_bart_bs8_4ep_lr3e-05__from:history-title-grounding___to:response,\n",
      "save_on_each_node=False,\n",
      "save_steps=80000,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "06/14/2023 16:51:59 - WARNING - datasets.builder - Using custom data configuration default-15b3ef04a8eeedeb\n",
      "06/14/2023 16:51:59 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "06/14/2023 16:51:59 - INFO - datasets.info - Loading Dataset info from /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-15b3ef04a8eeedeb/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e\n",
      "06/14/2023 16:51:59 - WARNING - datasets.builder - Reusing dataset csv (/home/aschernyavskiy/.cache/huggingface/datasets/csv/default-15b3ef04a8eeedeb/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n",
      "06/14/2023 16:51:59 - INFO - datasets.info - Loading Dataset info from /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-15b3ef04a8eeedeb/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 239.25it/s]\n",
      "[INFO|configuration_utils.py:644] 2023-06-14 16:52:00,086 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /home/aschernyavskiy/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "[INFO|configuration_utils.py:680] 2023-06-14 16:52:00,087 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-06-14 16:52:02,092 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/vocab.json from cache at /home/aschernyavskiy/.cache/huggingface/transformers/43978bdeaa326572886b44fcfed82f932f76571095ce31973e51c3da8ccade7f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-06-14 16:52:02,093 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/merges.txt from cache at /home/aschernyavskiy/.cache/huggingface/transformers/3c167ed8af56e6605eeb794b63a79d65d85e6708c9b04408d41946337030f5cd.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-06-14 16:52:02,093 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-06-14 16:52:02,093 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-06-14 16:52:02,093 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1771] 2023-06-14 16:52:02,093 >> loading file https://huggingface.co/facebook/bart-base/resolve/main/tokenizer.json from cache at /home/aschernyavskiy/.cache/huggingface/transformers/a878fcd69bba037c9b1b227f4213579ae43d0aaa9374e167bc6c5f41b1cfeb30.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|configuration_utils.py:644] 2023-06-14 16:52:02,340 >> loading configuration file https://huggingface.co/facebook/bart-base/resolve/main/config.json from cache at /home/aschernyavskiy/.cache/huggingface/transformers/f5310d276a6d1648d00c32fadc8bf7b4607e0fbd5b404fc4a0045960aa2bdfdb.a243ed957122436adb0b8d8e9d20f896f45c174b6324d625ca0a20a84f72a910\n",
      "[INFO|configuration_utils.py:680] 2023-06-14 16:52:02,343 >> Model config BartConfig {\n",
      "  \"_name_or_path\": \"facebook/bart-base\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 12,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1427] 2023-06-14 16:52:03,068 >> loading weights file https://huggingface.co/facebook/bart-base/resolve/main/pytorch_model.bin from cache at /home/aschernyavskiy/.cache/huggingface/transformers/486355ec722ef05fd480e999d4c763be56549ae930f6a3742ee721a5d2a05647.f2f355ad2775769afc60592b43a46d72ca548375e3a1d65f381a751e711cbadd\n",
      "CUSTOM BART\n",
      "[INFO|modeling_utils.py:1694] 2023-06-14 16:52:06,569 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:1703] 2023-06-14 16:52:06,569 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "[INFO|tokenization_utils_base.py:888] 2023-06-14 16:52:06,574 >> Assigning ['<u10>', '<u11>', '<u12>', '<u13>', '<u14>', '<u15>', '<u16>', '<u17>', '<u18>', '<u19>', '<u1>', '<u20>', '<u21>', '<u22>', '<u23>', '<u24>', '<u25>', '<u26>', '<u27>', '<u28>', '<u29>', '<u2>', '<u30>', '<u31>', '<u32>', '<u33>', '<u34>', '<u35>', '<u36>', '<u3>', '<u4>', '<u5>', '<u6>', '<u7>', '<u8>', '<u9>', '<to:u10>', '<to:u11>', '<to:u12>', '<to:u13>', '<to:u14>', '<to:u15>', '<to:u16>', '<to:u17>', '<to:u18>', '<to:u19>', '<to:u1>', '<to:u20>', '<to:u21>', '<to:u22>', '<to:u23>', '<to:u24>', '<to:u25>', '<to:u26>', '<to:u27>', '<to:u28>', '<to:u29>', '<to:u2>', '<to:u30>', '<to:u31>', '<to:u32>', '<to:u33>', '<to:u34>', '<to:u35>', '<to:u36>', '<to:u3>', '<to:u4>', '<to:u5>', '<to:u6>', '<to:u7>', '<to:u8>', '<to:u9>', '<s10>', '<s11>', '<s12>', '<s13>', '<s14>', '<s15>', '<s16>', '<s17>', '<s18>', '<s19>', '<s1>', '<s20>', '<s21>', '<s22>', '<s2>', '<s3>', '<s4>', '<s5>', '<s6>', '<s7>', '<s8>', '<s9>', '<Negative>', '<Neutral>', '<Positive>', '<negativereaction>', '<other>', '<appreciation>', '<unk>', '<elaboration>', '<answer>', '<question>', '<humor>', '<announcement>', '<agreement>', '<disagreement>', '<init>'] to the additional_special_tokens key of the tokenizer\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,575 >> Adding <u10> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,575 >> Adding <u11> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,575 >> Adding <u12> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,575 >> Adding <u13> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,575 >> Adding <u14> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,575 >> Adding <u15> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,575 >> Adding <u16> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,575 >> Adding <u17> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,575 >> Adding <u18> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,575 >> Adding <u19> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,575 >> Adding <u1> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,575 >> Adding <u20> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,575 >> Adding <u21> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,576 >> Adding <u22> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,576 >> Adding <u23> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,579 >> Adding <u24> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,580 >> Adding <u25> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,580 >> Adding <u26> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,580 >> Adding <u27> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,580 >> Adding <u28> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,580 >> Adding <u29> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,580 >> Adding <u2> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,580 >> Adding <u30> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,580 >> Adding <u31> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,580 >> Adding <u32> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,580 >> Adding <u33> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,580 >> Adding <u34> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,581 >> Adding <u35> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,581 >> Adding <u36> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,581 >> Adding <u3> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,581 >> Adding <u4> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,581 >> Adding <u5> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,581 >> Adding <u6> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,581 >> Adding <u7> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,581 >> Adding <u8> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,581 >> Adding <u9> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,581 >> Adding <to:u10> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,581 >> Adding <to:u11> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,581 >> Adding <to:u12> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,581 >> Adding <to:u13> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,581 >> Adding <to:u14> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,582 >> Adding <to:u15> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,582 >> Adding <to:u16> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,582 >> Adding <to:u17> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,582 >> Adding <to:u18> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,582 >> Adding <to:u19> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,582 >> Adding <to:u1> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,582 >> Adding <to:u20> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,582 >> Adding <to:u21> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,582 >> Adding <to:u22> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,582 >> Adding <to:u23> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,582 >> Adding <to:u24> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,582 >> Adding <to:u25> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,582 >> Adding <to:u26> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,582 >> Adding <to:u27> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,583 >> Adding <to:u28> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,583 >> Adding <to:u29> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,583 >> Adding <to:u2> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,583 >> Adding <to:u30> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,583 >> Adding <to:u31> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,583 >> Adding <to:u32> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,583 >> Adding <to:u33> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,583 >> Adding <to:u34> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,583 >> Adding <to:u35> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,583 >> Adding <to:u36> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,583 >> Adding <to:u3> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,583 >> Adding <to:u4> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,583 >> Adding <to:u5> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,583 >> Adding <to:u6> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,583 >> Adding <to:u7> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,584 >> Adding <to:u8> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,584 >> Adding <to:u9> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,584 >> Adding <s10> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,584 >> Adding <s11> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,584 >> Adding <s12> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,584 >> Adding <s13> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,584 >> Adding <s14> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,584 >> Adding <s15> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,584 >> Adding <s16> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,584 >> Adding <s17> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,584 >> Adding <s18> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,584 >> Adding <s19> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,584 >> Adding <s1> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,584 >> Adding <s20> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,585 >> Adding <s21> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,585 >> Adding <s22> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,585 >> Adding <s2> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,585 >> Adding <s3> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,585 >> Adding <s4> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,585 >> Adding <s5> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,585 >> Adding <s6> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,585 >> Adding <s7> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,585 >> Adding <s8> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,585 >> Adding <s9> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,585 >> Adding <Negative> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,585 >> Adding <Neutral> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,585 >> Adding <Positive> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,585 >> Adding <negativereaction> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,585 >> Adding <other> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,586 >> Adding <appreciation> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,586 >> Adding <elaboration> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,586 >> Adding <answer> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,586 >> Adding <question> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,586 >> Adding <humor> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,586 >> Adding <announcement> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,586 >> Adding <agreement> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,586 >> Adding <disagreement> to the vocabulary\n",
      "[INFO|tokenization_utils.py:426] 2023-06-14 16:52:06,586 >> Adding <init> to the vocabulary\n",
      "[INFO|tokenization_utils_base.py:888] 2023-06-14 16:52:06,587 >> Assigning <s> to the bos_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-06-14 16:52:06,587 >> Assigning </s> to the eos_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-06-14 16:52:06,588 >> Assigning <unk> to the unk_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-06-14 16:52:06,588 >> Assigning </s> to the sep_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-06-14 16:52:06,588 >> Assigning <pad> to the pad_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-06-14 16:52:06,589 >> Assigning <s> to the cls_token key of the tokenizer\n",
      "[INFO|tokenization_utils_base.py:888] 2023-06-14 16:52:06,589 >> Assigning <mask> to the mask_token key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/14/2023 16:52:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-15b3ef04a8eeedeb/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-04ccaee187bb19b7.arrow\n",
      "Running tokenizer on validation dataset:   0%|            | 0/4 [00:00<?, ?ba/s]06/14/2023 16:52:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/aschernyavskiy/.cache/huggingface/datasets/csv/default-15b3ef04a8eeedeb/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-ea78b9989ee1fba3.arrow\n",
      "Running tokenizer on validation dataset: 100%|████| 4/4 [00:18<00:00,  4.63s/ba]\n"
     ]
    }
   ],
   "source": [
    "# change special tokens map path in run_summarization.py\n",
    "!CUDA_VISIBLE_DEVICES=0 python custom_bart_scripts/run_summarization.py \\\n",
    "    --model_name_or_path=\"facebook/bart-base\" \\\n",
    "    --train_file=$train_fn \\\n",
    "    --validation_file=$val_fn \\\n",
    "    --text_column=$text_column \\\n",
    "    --summary_column=$target_col \\\n",
    "    --max_source_length=$max_source_length \\\n",
    "    --max_target_length=$max_target_length \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --per_device_train_batch_size=$batch_size \\\n",
    "    --per_device_eval_batch_size=$batch_size \\\n",
    "    --gradient_accumulation_steps=$gradient_accumulation_steps \\\n",
    "    --learning_rate=$learning_rate \\\n",
    "    --save_steps=80000 \\\n",
    "    --num_train_epochs=$n_epochs \\\n",
    "    --output_dir=$checkpoint_path \\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'custom_bart_scripts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_custom_bart import BartForConditionalGeneration\n",
    "from transformers import BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_top(text, num_beams=4,  max_source_len=1024, max_target_length=64, temperature=1.,\n",
    "                 do_sample=False, top_k=50, top_p=1, num_return_sequences=1, force_words_ids=None):\n",
    "    inputs = tokenizer([text], max_length=max_source_len, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], do_sample=do_sample, num_beams=num_beams,\n",
    "                             max_length=max_target_length, top_k=top_k, top_p=top_p, temperature=temperature,\n",
    "                             num_return_sequences=num_return_sequences)\n",
    "    \n",
    "    pred = tokenizer.batch_decode(summary_ids, clean_up_tokenization_spaces=False)\n",
    "    pred = [re.sub(r'\\s+', ' ', p).replace('</s>', '').replace('<s>', '').replace('<pad>', '').strip() for p in pred]\n",
    "    if len(pred) == 1:\n",
    "        return pred[0]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name_or_path = 'checkpoint/base_bart_copy1_bs8_7ep_lr3e-05__from:history_aug-title-grounding___to:response_aug'\n",
    "#model_name_or_path = 'checkpoint/base_bart_bs8_4ep_lr3e-05__from:history-title___to:response'\n",
    "model_name_or_path = 'checkpoint/base_bart_bs8_4ep_lr3e-05__from:history-title-grounding___to:response'\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM BART\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(model_name_or_path)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name_or_path).train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50373, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50373, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50373, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50373, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "# with open('bart_input/special_tokens_map_reddit_dial.pkl', 'rb') as f:\n",
    "#     special_tokens_dict = pickle.load(f)\n",
    "# num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set by hands using the results above\n",
    "source_lens = {\n",
    "    'history': 600,\n",
    "    'history_aug': 600,\n",
    "    'history_amr': 1024,\n",
    "    'history_discourse': 40,\n",
    "    'addr_amr': 300,\n",
    "    'response': 160,\n",
    "    'response_aug':  160,\n",
    "    'grounding': 850,\n",
    "    'title': 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_cols = model_name_or_path.split('/')[1].split('__')[1][5:].split('-')\n",
    "text_column = '@'.join(source_cols)\n",
    "target_col = model_name_or_path.split('/')[1].split('__')[-1][4:]\n",
    "max_encoder_length = 512\n",
    "\n",
    "max_source_length = min(1024, sum([source_lens[c] for c in source_cols]))\n",
    "max_target_length = source_lens[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_source_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_fn = f'bart_input/val_reddit_dial_df_base__{\"-\".join(source_cols)}__{target_col}.csv'\n",
    "test_data = pd.read_csv(val_fn, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "<s1> <u1> <to:u1> <init> <Negative> What about the people who have genuine concerns? Surely not everyone who voiced complaints were disrespectful and rude.Why piss away a good Superman to do a younger one? Why? I genuinely don't get it.I'm not going to harass or bully James Gunn or anyone else, but I am calling a spade a spade in that this Henry Cavill situation is a bunch of bullshit. </s> <s2> <u2> <to:u1> <question> <Neutral> What is there to not get? Henry is 40. The new Superman movie won’t be ready for a few years. Gunn is in charge, and his plan is to tell a story with a younger Superman. That’s all there is to it. </s> <s1> <u3> <to:u2> <question> <Neutral> Why a younger Superman? That's what I don't get. Why?Does Henry Cavill look like his body is going to wither away in 10 years? He is pretty healthy for a 40 year old. You could digitally deage him to play a younger Superman. </s> <s3> <u4> <to:u3> <answer> <Neutral> My guess It’s about longevity. Yes, they can digitally alter their age but even Robert Downey tapped out after 10 yrs. If they start with a young Superman, they can potentially have a story that spans over quite a long time. </s> <s1> <u5> <to:u4> <question> <Negative> I understand that. But why keep other members of the Justice League if they just recast Superman? Gunn kinda makes it seem like that is an option.I hate a younger Superman, but why not just recast everybody at that point? </s> <s3> <u6> <to:u5> </s> James Gunn’s latest Instagram post </s> The Suicide Squad is a 2021 American superhero film based on the DC Comics team Suicide Squad. Produced by DC Films, Atlas Entertainment, and The Safran Company, and distributed by Warner Bros. Pictures, it is a standalone sequel to Suicide Squad (2016) and the 10th film in the DC Extended Universe (DCEU). It was written and directed by James Gunn and stars an ensemble cast including Margot Robbie, Idris Elba, John Cena, Joel Kinnaman, Sylvester Stallone, Viola Davis, David Dastmalchian, Daniela Melchior, Michael Rooker, Jai Courtney, Peter Capaldi, Alice Braga, and Pete Davidson. In the film, several convicts join a task force known as the \"Suicide Squad\" in exchange for lighter sentences. Guardians of the Galaxy Vol. 3 (stylized as Guardians of the Galaxy Volume 3) is an upcoming American superhero film based on the Marvel Comics superhero team Guardians of the Galaxy, produced by Marvel Studios, and distributed by Walt Disney Studios Motion Pictures. It is intended to be the sequel to Guardians of the Galaxy (2014) and Guardians of the Galaxy Vol. 2 (2017), and the 32nd film in the Marvel Cinematic Universe (MCU). The film is written and directed by James Gunn and stars an ensemble cast featuring Chris Pratt, Zoe Saldaña, Dave Bautista, Karen Gillan, Pom Klementieff, Vin Diesel, Bradley Cooper, Sean Gunn, Chukwudi Iwuji, Will Poulter, Elizabeth Debicki, Maria Bakalova, and Sylvester Stallone. Guardians of the Galaxy Vol. 2 is a 2017 American superhero film based on the Ma\n",
      "\n",
      "Prediction: <elaboration> <Neutral> I don't understand why they would want to do a younger Superman.\n",
      "\n",
      "\n",
      "GT: <answer> <Neutral> You’re acting like we know who is staying and who isn’t. We literally know nothing, just what Gunn tweets and what we hear from the rumor mill. What I mean by that is, we don’t know what the casting looks like yet. Also, we have no idea what’s happening behind the scenes. Do these actors want to stay after what DC put them through over the past 10 yrs? I’m sure some are game, but I can definitely see Affleck and Gadot not wanting to waste their time. I say we hold on the pitchforks until the first movie comes out.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "110\n",
      "<s1> <u1> <to:u1> <init> <Negative> Not everyone is like that though. There are plenty of people who still agree that rebooting is probably a bad choice. Like how are you gonna find a better Superman now? What're you going to do? That shitty fancast like Austin Butler? Tom Holland? Tom Hanks? </s> <s2> <u2> <to:u1> <answer> <Negative> That shitty fancast like Austin Butler? Tom Holland? Tom Hanks?Yes those are the only three actors in the world. </s> <s1> <u3> <to:u2> </s> James Gunn’s latest Instagram post </s> The Suicide Squad is a 2021 American superhero film based on the DC Comics team Suicide Squad. Produced by DC Films, Atlas Entertainment, and The Safran Company, and distributed by Warner Bros. Pictures, it is a standalone sequel to Suicide Squad (2016) and the 10th film in the DC Extended Universe (DCEU). It was written and directed by James Gunn and stars an ensemble cast including Margot Robbie, Idris Elba, John Cena, Joel Kinnaman, Sylvester Stallone, Viola Davis, David Dastmalchian, Daniela Melchior, Michael Rooker, Jai Courtney, Peter Capaldi, Alice Braga, and Pete Davidson. In the film, several convicts join a task force known as the \"Suicide Squad\" in exchange for lighter sentences. Guardians of the Galaxy Vol. 3 (stylized as Guardians of the Galaxy Volume 3) is an upcoming American superhero film based on the Marvel Comics superhero team Guardians of the Galaxy, produced by Marvel Studios, and distributed by Walt Disney Studios Motion Pictures. It is intended to be the sequel to Guardians of the Galaxy (2014) and Guardians of the Galaxy Vol. 2 (2017), and the 32nd film in the Marvel Cinematic Universe (MCU). The film is written and directed by James Gunn and stars an ensemble cast featuring Chris Pratt, Zoe Saldaña, Dave Bautista, Karen Gillan, Pom Klementieff, Vin Diesel, Bradley Cooper, Sean Gunn, Chukwudi Iwuji, Will Poulter, Elizabeth Debicki, Maria Bakalova, and Sylvester Stallone. Guardians of the Galaxy Vol. 2 is a 2017 American superhero film based on the Marvel Comics superhero team Guardians of the Galaxy, produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures. It is the sequel to Guardians of the Galaxy (2014) and the 15th film in the Marvel Cinematic Universe (MCU). Written and directed by James Gunn, the film stars an ensemble cast featuring Chris Pratt, Zoe Saldaña, Dave Bautista, Vin Diesel, Bradley Cooper, Michael Rooker, Karen Gillan, Pom Klementieff, Sylvester Stallone, and Kurt Russell. In the film, the Guardians travel throughout the cosmos as they help Peter Quill learn more about his mysterious parentage. Guardians of the Galaxy (retroactively referred to as Guardians of the Galaxy Vol. 1) is a 2014 American superhero film based on the Marvel Comics superhero team of the same name. Produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures, it is the 10th film in the Marvel Cinematic Universe (MCU). Directed by James Gunn, who wrote the screenplay with Nicole\n",
      "\n",
      "Prediction: <elaboration> <Negative> I don't think that's a bad choice.\n",
      "\n",
      "\n",
      "GT: <unk> <Neutral> I just through those last 2 in as a joke. But really it's Conan be really hard to find a good Superman. They better be trying extra hard to find one.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "120\n",
      "<s1> <u1> <to:u1> <init> <Neutral> deleted </s> <s2> <u2> <to:u1> <question> <Neutral> Huh? </s> <s1> <u3> <to:u2> <answer> <Neutral> removed </s> <s3> <u4> <to:u1> </s> James Gunn’s latest Instagram post </s> The Suicide Squad is a 2021 American superhero film based on the DC Comics team Suicide Squad. Produced by DC Films, Atlas Entertainment, and The Safran Company, and distributed by Warner Bros. Pictures, it is a standalone sequel to Suicide Squad (2016) and the 10th film in the DC Extended Universe (DCEU). It was written and directed by James Gunn and stars an ensemble cast including Margot Robbie, Idris Elba, John Cena, Joel Kinnaman, Sylvester Stallone, Viola Davis, David Dastmalchian, Daniela Melchior, Michael Rooker, Jai Courtney, Peter Capaldi, Alice Braga, and Pete Davidson. In the film, several convicts join a task force known as the \"Suicide Squad\" in exchange for lighter sentences. Guardians of the Galaxy Vol. 3 (stylized as Guardians of the Galaxy Volume 3) is an upcoming American superhero film based on the Marvel Comics superhero team Guardians of the Galaxy, produced by Marvel Studios, and distributed by Walt Disney Studios Motion Pictures. It is intended to be the sequel to Guardians of the Galaxy (2014) and Guardians of the Galaxy Vol. 2 (2017), and the 32nd film in the Marvel Cinematic Universe (MCU). The film is written and directed by James Gunn and stars an ensemble cast featuring Chris Pratt, Zoe Saldaña, Dave Bautista, Karen Gillan, Pom Klementieff, Vin Diesel, Bradley Cooper, Sean Gunn, Chukwudi Iwuji, Will Poulter, Elizabeth Debicki, Maria Bakalova, and Sylvester Stallone. Guardians of the Galaxy Vol. 2 is a 2017 American superhero film based on the Marvel Comics superhero team Guardians of the Galaxy, produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures. It is the sequel to Guardians of the Galaxy (2014) and the 15th film in the Marvel Cinematic Universe (MCU). Written and directed by James Gunn, the film stars an ensemble cast featuring Chris Pratt, Zoe Saldaña, Dave Bautista, Vin Diesel, Bradley Cooper, Michael Rooker, Karen Gillan, Pom Klementieff, Sylvester Stallone, and Kurt Russell. In the film, the Guardians travel throughout the cosmos as they help Peter Quill learn more about his mysterious parentage. Guardians of the Galaxy (retroactively referred to as Guardians of the Galaxy Vol. 1) is a 2014 American superhero film based on the Marvel Comics superhero team of the same name. Produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures, it is the 10th film in the Marvel Cinematic Universe (MCU). Directed by James Gunn, who wrote the screenplay with Nicole Perlman, it features an ensemble cast including Chris Pratt, Zoe Saldaña, Dave Bautista, Vin Diesel, and Bradley Cooper as the titular Guardians, along with Lee Pace, Michael Rooker, Karen Gillan, Djimon Hounsou, John C. Reilly, Glenn Close, and Benicio del Toro. In the film, Peter Quill and a group of ext\n",
      "\n",
      "Prediction: <unk> <Neutral> removed\n",
      "\n",
      "\n",
      "GT: <unk> <Neutral> I am reading no lies with that tweet.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "<s1> <u1> <to:u1> <init> <Neutral> This is the part of Superman that Snyder never understood. He was so obsessed with Superman’s power and the implications of it on the world, and neglected Superman’s HUMANITY. He may come from another planet but the entire point of Superman is that he is human, and a kind one at that.Snyder’s approach where he treated these characters as gods instead of people played a huge role in why audiences struggled to connect with them. There was far more attention paid to “what do people think of Superman” and not enough attention paid to “what does Superman do for people”I want James Gunn’s new Superman to focus far more on Clark Kent than the deity that is Superman, at least for his first appearance. Take a look at the first Captain America film. We spend most of it with Steve Rogers before the super-serum, and even then we can see his kindness, his bravery, and his loyalty to others. His “I don’t like bullies” moment was just the perfect way for audiences to connect with him and understand why he is deserving of the title of Captain America. We need something similar for the new Superman. </s> <s2> <u2> <to:u1> <elaboration> <Neutral> Snyder didn't treat Superman like a deity. He made Jesus comparisons, but that's nothing new and a reasonable comparison to make. I've heard people compare themselves to Jesus in real life, compare their leaders to Jesus in real life, and Superman is someone who would \"definitely be compared to Jesus in real life and would \"definitely\" have others make that comparison, but wouldn't make it himself. Which is how he is in Snyder's movies. His humanity was at the forefront of his portrayal, especially when he was standing against the other Kryptonians, who almost came off like robots or replicants who couldn't fight their programming. Also, the moment Cavill is describing with kid would've come from the role he played in Snyder's movies. </s> <s3> <u3> <to:u1> <other> <Neutral> removed </s> <s2> <u4> <to:u1> <disagreement> <Neutral> Wasn't the whole fiasco with the government, the people of America not accepting Superman kinda similar to Jesus's story?You'd have to strip their stories down to make them basically the same, and you can do that with pretty much anyone (which is how so many people compare themselves and their leaders to Jesus). Snyder didn't treat Superman like a deity, Snyder pointed out people in the world might see him like a deity, and that different groups would respond in different ways (as many people in governments believe in God/Jesus, they would likely have a problem if either actually showed up and started giving orders), while he himself is actually just a guy trying to do the right thing. There would be people deifying him like in the Day of the Dead scene, people projecting their issues with God and religion onto him like Lex Luthor, etc. Meanwhile, Superman is just this guy over trying to live his life and help people. </s> <s3> <u5> <to:u1> <other> <Neutral> rem\n",
      "\n",
      "Prediction: <unk> <Neutral> I think it's a shame that WB and directors didn't though\n",
      "\n",
      "\n",
      "GT: <elaboration> <Neutral> Maybe because there are already 5 other Superman movies that don't focus on it, and Snyder didn't think they needed to rehash what had already been done in the last 5 films and 15 years' worth of live action television, with which I'd agree. I've already seen the ice crystal fortress of solitude, land-development Lex, all white iced out krypton, mass adulation of Superman, and the personas he projects as Clark and Superman. I think they just took for granted that audience was already familiar enough with how the world generally sees him to not need to see it again, so they just have characters talking about it while the film spends time on the other side of things. Granted, I do think BvS came from a story written for MoS2 that was rewritten to be a shared-universe starter. If you take out all the JL stuff, it looks like Keef was on his way to becoming Metallo made by Lex, Lois' investigation would've led her to them, Olson's role probably wouldn't have been reduced, etc.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "140\n",
      "<s1> <u1> <to:u1> <init> <Negative> Still too old for Gunn's taste. </s> <s2> <u2> <to:u1> <answer> <Neutral> Yep, he’s looking for a young boy </s> <s3> <u3> <to:u1> </s> With talk about the new Superman being played by a younger actor, I can’t help but think this kid would be a great choice. </s> Batman v Superman: Dawn of Justice is a 2016 American superhero film based on the DC Comics characters Batman and Superman. Distributed by Warner Bros., it is a follow-up to the 2013 film Man of Steel and the second film in the DC Extended Universe (DCEU). The film was directed by Zack Snyder, written by Chris Terrio and David S. Goyer, and features an ensemble cast that includes Ben Affleck as Batman and Henry Cavill as Superman, alongside Amy Adams, Jesse Eisenberg, Diane Lane, Laurence Fishburne, Jeremy Irons, Holly Hunter, and Gal Gadot. Batman v Superman: Dawn of Justice is the first live-action film to feature Batman and Superman together, as well as the first live-action cinematic portrayal of Wonder Woman. In the film, criminal mastermind Lex Luthor manipulates Batman into a preemptive battle with Superman, who Luthor is obsessed with destroying. Man of Steel is a 2013 superhero film based on the DC Comics character Superman. Directed by Zack Snyder from a screenplay by David S. Goyer, it is the first installment in the DC Extended Universe (DCEU) and a reboot of the Superman film series, depicting the character's origin story. It stars Henry Cavill in the title role along with Amy Adams, Michael Shannon, Kevin Costner, Diane Lane, Laurence Fishburne, and Russell Crowe. In the film, Clark Kent learns that he is a superpowered alien from the planet Krypton. He assumes the role of mankind's protector as Superman, making the choice to face General Zod and prevent him from destroying humanity. Star Wars (retroactively titled Star Wars: Episode IV – A New Hope) is a 1977 American epic space opera film written and directed by George Lucas, produced by Lucasfilm and distributed by 20th Century Fox. It is the first film in the Star Wars film series and fourth chronological chapter of the \"Skywalker Saga\". Set \"a long time ago\" in a fictional universe where the galaxy is ruled by the tyrannical Galactic Empire, the story focuses on a group of freedom fighters known as the Rebel Alliance, who aim to destroy the Empire's newest weapon, the Death Star. Luke Skywalker becomes caught in the conflict while learning the ways of a metaphysical power known as \"the Force\" from Jedi Master Obi-Wan Kenobi. The cast includes Mark Hamill, Harrison Ford, Carrie Fisher, Peter Cushing, Alec Guinness, David Prowse, James Earl Jones, Anthony Daniels, Kenny Baker, and Peter Mayhew. Batman v Superman: Dawn of Justice is a 2016 American superhero film based on the DC Comics characters Batman and Superman. Distributed by Warner Bros., it is a follow-up to the 2013 film Man of Steel and the second film in the DC Extended Universe (DCEU). The film was directed by Zack Snyder, w\n",
      "\n",
      "Prediction: <elaboration> <Neutral> I think he’s looking for a young boy.\n",
      "\n",
      "\n",
      "GT: <answer> <Neutral> Maybe they should cast kid named paint\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "150\n",
      "<s1> <u1> <to:u1> <init> <Neutral> People laughed out loud in the cinemas at this scene but I don’t think Matt Reeves intended for that reaction </s> <s2> <u2> <to:u1> <answer> <Negative> I guess it depends on your sense of humor. I could see people laughing at an adult having a tantrum like he's a baby. </s> <s3> <u3> <to:u2> <agreement> <Neutral> Yeah Riddler was definitely written in a way that you're meant to laugh at him. The whole \"hey guys welcome to my YouTube channel\" streamer scene is proof of that so I think in a scene like this we're meant to feel ridicule towards him. </s> <s1> <u4> <to:u3> </s> The Rock and Henry Cavill after their meeting with James Gunn </s> The DC Extended Universe (DCEU) is an American media franchise and shared universe centered on a series of superhero films and television series produced by DC Studios and distributed by Warner Bros. Pictures. It is based on characters that appear in American comic books published by DC Comics. The DCEU also includes comic books, short films, novels, and video games. Like the original DC Universe in comic books, the DCEU was established by crossing over common plot elements, settings, cast, and characters. Created by Jerry Siegel and Joe Shuster in June 1938, DC Comics' Superman has appeared in various films almost since his inception. He debuted in cinemas in a series of animated shorts beginning in 1941, subsequently starring in two movie serials in 1948 and 1950. An independent studio, Lippert Pictures, released the first Superman feature film, Superman and the Mole Men, starring George Reeves, in 1951. In 1974, the film rights to the Superman character were purchased by Ilya Salkind, Alexander Salkind, and Pierre Spengler. After numerous scripts, Richard Donner was hired as their director, filming Superman (1978) and Superman II (1980) simultaneously. Man of Steel is a 2013 superhero film based on the DC Comics character Superman. Directed by Zack Snyder from a screenplay by David S. Goyer, it is the first installment in the DC Extended Universe (DCEU) and a reboot of the Superman film series, depicting the character's origin story. It stars Henry Cavill in the title role along with Amy Adams, Michael Shannon, Kevin Costner, Diane Lane, Laurence Fishburne, and Russell Crowe. In the film, Clark Kent learns that he is a superpowered alien from the planet Krypton. He assumes the role of mankind's protector as Superman, making the choice to face General Zod and prevent him from destroying humanity. The DC Extended Universe (DCEU) is an American media franchise and shared universe centered on a series of superhero films and television series produced by DC Studios and distributed by Warner Bros. Pictures. It is based on characters that appear in American comic books published by DC Comics. The DCEU also includes comic books, short films, novels, and video games. Like the original DC Universe in comic books, the DCEU was established by crossing over common plot elements, setting\n",
      "\n",
      "Prediction: <elaboration> <Neutral> I think it depends on your sense of humor.\n",
      "\n",
      "\n",
      "GT: <elaboration> <Negative> No it’s meant to come across as threatening and disturbing. The music in films is there to emphasise what the director wants you to be feeling. Watch that scene and just let the music play out, this scene is meant to be scary and it just isn’t because Riddler comes across as an autistic child.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "160\n",
      "<s1> <u1> <to:u1> <init> <Neutral> Suppose he's playing it safe, it's what's \"worked\" for him in regards to the DCEU. Still, Full Reboot is what it says on the tin lol, not \"Full reboot but errr i'll keep 4-5 characters\" </s> <s2> <u2> <to:u1> <answer> <Neutral> But he ain’t say full reboot. He said he’s keeping what works and moving on with what didn’t </s> <s3> <u3> <to:u1> <answer> <Neutral> Sounds more and more like \"keeping what works\" means keeping his projects. </s> <s4> <u4> <to:u3> <agreement> <Positive> What exactly was \"working\" before? Maybe TSS didn't make a lot of money (not getting into that as it's a tired argument at this point) but both that and Peacemaker were the first DC projects in a WHILE that got people talking and interested. I'd say this is a much better direction than the crap we were given previously. Two movies that were decent and the rest was extremely mixed opinions or a complete let down. </s> <s3> <u5> <to:u4> <unk> <Neutral> Wonder Woman, Shazam, Aquaman most of the snyderverse cast. </s> <s4> <u6> <to:u5> </s> Imagine if James Wan took control of DC and fired everyone except his Aquaman cast </s> Many films since the 1980s and earlier have featured mid- and post-credits scenes, also known as credit cookies. Such scenes often include comedic gags, plot revelations, outtakes, and/or hints about sequels. Many films since the 1980s and earlier have featured mid- and post-credits scenes, also known as credit cookies. Such scenes often include comedic gags, plot revelations, outtakes, and/or hints about sequels.\n",
      "\n",
      "Prediction: <elaboration> <Neutral> I think he’s just saying he‘s keeping what works and moving on with what didn’t.\n",
      "\n",
      "\n",
      "GT: <unk> <Negative> Oh you’re just an insane Snyder guy. Got it. Rnyderverse is over here. Go talk about that defunct universe elsewhere\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "<s1> <u1> <to:u1> <init> <Positive> The Suicide Squad and Peacemaker have beautiful scenes and moments. What ever Gunn does i trust him. </s> <s2> <u2> <to:u1> <answer> <Positive> Well, yeah. Don't get me wrong, I love the idea of Henry Cavill as Superman but James Gunn wrote better lines for a talking tree that only uses the words I, we, am, are, and Groot than Snyder did for one of the most important comics characters of all time. </s> <s3> <u3> <to:u1> <answer> <Positive> i love dialogues from the snyder trilogy : simple, classy and deep. a beautiful lie.no one stays good in this world. if there's one pourcent chance that he is our enemy, we have to take it as an absolute necessity. do you bleed ? you're will !!!!! you know what is the greatest lie ? that power can be innocent. lex luthor when he explain his motivation. superman was never real, it was just a dream from a farmer. must there be a Superman ? Men are still good, we fight, we kill, we betray each other but we can rebuild. we have to. same for zljla, so many good dialogue. change your future, Change your past, it's all right now. i'm not broken and i'm not alone. </s> <s4> <u4> <to:u1> <answer> <Positive> You missed one bro: \"if God is all good he cannot be all powerful and if he is all Powerful he cannot be all good\"This one was of the utmost brilliance. </s> <s5> <u5> <to:u1> </s> This is the only scene in a DC movie that actually made me cry. </s> Dirty Dancing is a 1987 American romantic drama dance film written by Eleanor Bergstein, produced by Linda Gottlieb, and directed by Emile Ardolino. Starring Patrick Swayze and Jennifer Grey, it tells the story of Frances \"Baby\" Houseman, a young woman who falls in love with dance instructor Johnny Castle (Swayze) at a vacation resort. The film was based on screenwriter Bergstein's own childhood. She originally wrote a screenplay for the Michael Douglas film It's My Turn, but ultimately ended up conceiving a story for a film which became Dirty Dancing. She finished the script in 1985, but management changes at Metro-Goldwyn-Mayer put the film in development hell. Batman v Superman: Dawn of Justice is a 2016 American superhero film based on the DC Comics characters Batman and Superman. Distributed by Warner Bros., it is a follow-up to the 2013 film Man of Steel and the second film in the DC Extended Universe (DCEU). The film was directed by Zack Snyder, written by Chris Terrio and David S. Goyer, and features an ensemble cast that includes Ben Affleck as Batman and Henry Cavill as Superman, alongside Amy Adams, Jesse Eisenberg, Diane Lane, Laurence Fishburne, Jeremy Irons, Holly Hunter, and Gal Gadot. Batman v Superman: Dawn of Justice is the first live-action film to feature Batman and Superman together, as well as the first live-action cinematic portrayal of Wonder Woman. In the film, criminal mastermind Lex Luthor manipulates Batman into a preemptive battle with Superman, who Luthor is obsessed with destroying. This is a list of chara\n",
      "\n",
      "Prediction: <elaboration> <Neutral> I think it's a good idea to have a talking tree.\n",
      "\n",
      "\n",
      "GT: <answer> <Neutral> im14andthisisdeep\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "180\n",
      "<s1> <u1> <to:u1> <init> <Positive> The Suicide Squad and Peacemaker have beautiful scenes and moments. What ever Gunn does i trust him. </s> <s2> <u2> <to:u1> <answer> <Negative> There’s no way this isn’t a bot LMFAO </s> <s3> <u3> <to:u1> </s> This is the only scene in a DC movie that actually made me cry. </s> Dirty Dancing is a 1987 American romantic drama dance film written by Eleanor Bergstein, produced by Linda Gottlieb, and directed by Emile Ardolino. Starring Patrick Swayze and Jennifer Grey, it tells the story of Frances \"Baby\" Houseman, a young woman who falls in love with dance instructor Johnny Castle (Swayze) at a vacation resort. The film was based on screenwriter Bergstein's own childhood. She originally wrote a screenplay for the Michael Douglas film It's My Turn, but ultimately ended up conceiving a story for a film which became Dirty Dancing. She finished the script in 1985, but management changes at Metro-Goldwyn-Mayer put the film in development hell. Batman v Superman: Dawn of Justice is a 2016 American superhero film based on the DC Comics characters Batman and Superman. Distributed by Warner Bros., it is a follow-up to the 2013 film Man of Steel and the second film in the DC Extended Universe (DCEU). The film was directed by Zack Snyder, written by Chris Terrio and David S. Goyer, and features an ensemble cast that includes Ben Affleck as Batman and Henry Cavill as Superman, alongside Amy Adams, Jesse Eisenberg, Diane Lane, Laurence Fishburne, Jeremy Irons, Holly Hunter, and Gal Gadot. Batman v Superman: Dawn of Justice is the first live-action film to feature Batman and Superman together, as well as the first live-action cinematic portrayal of Wonder Woman. In the film, criminal mastermind Lex Luthor manipulates Batman into a preemptive battle with Superman, who Luthor is obsessed with destroying. This is a list of characters from The Lego Movie franchise produced by Warner Animation Group and The Lego Group, which consists of the animated films (and LEGO sets from that film), 4D film and TV series: The Lego Movie (2014), The Lego Movie: 4D – A New Adventure (2016), and The Lego Movie 2: The Second Part (2019) as well as its spin-offs The Lego Batman Movie (2017), The Lego Ninjago Movie (2017), and Unikitty! (2017–2020), and the video games. == Introduced in The Lego Movie and The Lego Movie 2: The Second Part == === Emmet Brickowski === Emmet Brickowski (voiced by Chris Pratt in The Lego Movie, The Lego Movie 2: The Second Part and Lego Dimensions, A. J. Locascio in The Lego Movie: 4D – A New Adventure, The Lego Movie 2 Videogame and some promos, Keith Ferguson in The Lego Movie Videogame) is the main protagonist of The Lego Movie. He is a construction worker chosen by the prophecy to find the Piece of Resistance to save the Lego World as \"The Special\" and stop the evil Lord Business. Wyldstyle, who helped him defeat Lord Business, becomes his girlfriend at the end of the original film. Dirty Dancing is a 1987 American r\n",
      "\n",
      "Prediction: <answer> <Neutral> I’m not sure if it’s a bot LMFAO.\n",
      "\n",
      "\n",
      "GT: <question> <Neutral> Have uou ever considered that people have different taste in films than you?\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "190\n",
      "<s1> <u1> <to:u1> <init> <Neutral> When James Gunn made me care more for Ratcatcher 1 and 2 than Snyder did with Superman and Batman. </s> <s2> <u2> <to:u1> <answer> <Negative> You might need a refill on your brain medicine, LOL. Superman's death and funeral was one of the most moving scenes I've ever seen in a movie. Gunn's TSS characters were pathetic, boring and extremely poorly written. </s> <s3> <u3> <to:u2> <agreement> <Negative> Yup, that’s why majority of people hated it </s> <s4> <u4> <to:u1> <question> <Positive> I'm fascinated by where your getting this \"majority\" </s> <s3> <u5> <to:u4> </s> This is the only scene in a DC movie that actually made me cry. </s> Dirty Dancing is a 1987 American romantic drama dance film written by Eleanor Bergstein, produced by Linda Gottlieb, and directed by Emile Ardolino. Starring Patrick Swayze and Jennifer Grey, it tells the story of Frances \"Baby\" Houseman, a young woman who falls in love with dance instructor Johnny Castle (Swayze) at a vacation resort. The film was based on screenwriter Bergstein's own childhood. She originally wrote a screenplay for the Michael Douglas film It's My Turn, but ultimately ended up conceiving a story for a film which became Dirty Dancing. She finished the script in 1985, but management changes at Metro-Goldwyn-Mayer put the film in development hell. Batman v Superman: Dawn of Justice is a 2016 American superhero film based on the DC Comics characters Batman and Superman. Distributed by Warner Bros., it is a follow-up to the 2013 film Man of Steel and the second film in the DC Extended Universe (DCEU). The film was directed by Zack Snyder, written by Chris Terrio and David S. Goyer, and features an ensemble cast that includes Ben Affleck as Batman and Henry Cavill as Superman, alongside Amy Adams, Jesse Eisenberg, Diane Lane, Laurence Fishburne, Jeremy Irons, Holly Hunter, and Gal Gadot. Batman v Superman: Dawn of Justice is the first live-action film to feature Batman and Superman together, as well as the first live-action cinematic portrayal of Wonder Woman. In the film, criminal mastermind Lex Luthor manipulates Batman into a preemptive battle with Superman, who Luthor is obsessed with destroying. This is a list of characters from The Lego Movie franchise produced by Warner Animation Group and The Lego Group, which consists of the animated films (and LEGO sets from that film), 4D film and TV series: The Lego Movie (2014), The Lego Movie: 4D – A New Adventure (2016), and The Lego Movie 2: The Second Part (2019) as well as its spin-offs The Lego Batman Movie (2017), The Lego Ninjago Movie (2017), and Unikitty! (2017–2020), and the video games. == Introduced in The Lego Movie and The Lego Movie 2: The Second Part == === Emmet Brickowski === Emmet Brickowski (voiced by Chris Pratt in The Lego Movie, The Lego Movie 2: The Second Part and Lego Dimensions, A. J. Locascio in The Lego Movie: 4D – A New Adventure, The Lego Movie 2 Videogame and some promos, Keith Ferguson in\n",
      "\n",
      "Prediction: <unk> <Negative> I'm fascinated by where your getting this \"majority\"\n",
      "\n",
      "\n",
      "GT: <elaboration> <Negative> Idk probably the poor general audience reception, and the fact that most fans said they felt nothing for the death of superman and thought it was wasted in the second movie\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(100, 200, 10):\n",
    "    input_text = test_data[text_column].values[idx]\n",
    "    input_texts = input_text.split(' </s> ')\n",
    "    pred = generate_top(input_text,\n",
    "                        num_beams=1,#2,\n",
    "                        max_source_len=max_source_length,\n",
    "                        max_target_length=max_target_length)\n",
    "    \n",
    "    \n",
    "    print(idx)\n",
    "    print(input_text[:3000])\n",
    "    print()\n",
    "    #for i, col in enumerate(source_cols):\n",
    "    #    print(f'{col}:\\n', input_texts[i][:3000])\n",
    "    #     print()\n",
    "    print('Prediction:', pred)\n",
    "    print('\\n\\nGT:', test_data[target_col].values[idx])\n",
    "    print('\\n' + '-'*70 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:06<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "num_same = 0\n",
    "for idx in tqdm(range(100, 300)):\n",
    "    input_text = test_data[text_column].values[idx]\n",
    "    \n",
    "    #try:\n",
    "    pred = generate_top(input_text,\n",
    "                        num_beams=1,\n",
    "                        max_source_len=max_source_length,\n",
    "                        max_target_length=max_target_length)\n",
    "    \n",
    "    gt = test_data[target_col].values[idx]\n",
    "    \n",
    "    if pred.split()[0] == gt.split()[0]:\n",
    "        num_same += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate predictions for full test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_fn = f'bart_input/val_reddit_dial_df_base_short__{\"-\".join(source_cols)}__{target_col}.csv'\n",
    "test_data = pd.read_csv(val_fn, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14615/15716 [1:25:38<07:47,  2.35it/s]  "
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "\n",
    "for idx in tqdm(range(len(test_data))):\n",
    "    input_text = test_data[text_column].values[idx]\n",
    "    try:\n",
    "        pred = generate_top(input_text,\n",
    "                            num_beams=1,\n",
    "                            max_source_len=max_source_length,\n",
    "                            max_target_length=max_target_length,\n",
    "                            do_sample=True)\n",
    "    except:\n",
    "        pred = \"\"\n",
    "        \n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_name_or_path.replace('checkpoint/', 'predictions/').replace('/checkpoint', '_') + '_short.pkl', 'wb') as f:\n",
    "    pickle.dump(preds, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path.replace('checkpoint/', 'predictions_v2/').replace('/checkpoint', '_') + '.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
